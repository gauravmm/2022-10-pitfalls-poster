Divergence from the counterintuitive behavior of regularization in TD also occurs in neural networks. We plot relationship between TD error and regularization over 100 separate initializations below:
\begin{center}
    \hspace*{1in}
    \includegraphics[scale=0.4]{parts/nn/nn.png}
\end{center}
All initializations exhibit a bump in error with similar amounts of regularization, showing that this divergence is not an artifact of poor initialization.
