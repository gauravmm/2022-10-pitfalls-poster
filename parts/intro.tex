% Highlight Box
\definecolor{teal}{rgb}{0.99,0.37,0.53}
\newmdenv[innerlinewidth=0.5pt, roundcorner=4pt,linecolor=teal,backgroundcolor=teal,innerleftmargin=6pt,
    innerrightmargin=6pt,innertopmargin=6pt,innerbottommargin=6pt,leftmargin=-9pt,rightmargin=-9pt]{mybox}

Temporal Difference (TD) learning is the process of estimating the value of each state in a Markov Reward Process (MRP) using the differences in the value of successive states. This is done with the Bellman equation:
\begin{center}
    \includegraphics[scale=0.4]{parts/intro/bellman}
\end{center}
TD may learn arbitrarily bad value functions under the \emph{Deadly Triad} conditions:
\begin{enumerate}
    \item Bootstrapping (learning from your own output)
    \item Function approximation
    \item Off-policy updates
\end{enumerate}
\begin{center}
    \includegraphics[scale=0.4]{parts/intro/threestatedivergence}
\end{center}
A popular mitigation strategy in the literature is to \emph{regularize} the learned parameters, commonly by penalizing their magnitude. Regularization is supposed to work like this:
\begin{center}
    \includegraphics[scale=0.4]{parts/intro/regworks}
\end{center}
However, in this work we show that:
\begin{mybox}
    {\headerfont Regularization doesn't always work!}
    \vspace{.5em}
    {\large
        \begin{enumerate}
            \item Models may be \emph{vacuous}.
            \item Regularizing TD is \emph{not monotonic}.
            \item Emphatic models are \emph{not immune}.
        \end{enumerate}
    }
    \vspace{.5em}
\end{mybox}